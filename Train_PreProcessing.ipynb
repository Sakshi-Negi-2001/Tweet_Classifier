{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from string import punctuation\n",
    "from time import process_time\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import unicodedata\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess_Data():\n",
    "    \n",
    "    # ----------------------------------------- Constructor -----------------------------------------\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.punctuation = set(punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.SMILEY)\n",
    "        self.stopword_list = set(stopwords.words('english'))\n",
    "        unwanted_stopwords = {'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'couldn', 'what', 'which', 'who',\n",
    "                              'whom', 'why', 'how', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\n",
    "                              \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn',\n",
    "                              \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn',\n",
    "                              \"shouldn't\", 'wasn',\"wasn't\",'weren', \"weren't\", 'won', \"won't\", 'wouldn',\n",
    "                              \"wouldn't\", 'don', \"don't\"}\n",
    "\n",
    "        self.stopword_list = [x for x in self.stopword_list if x not in unwanted_stopwords]\n",
    "       \n",
    "    \n",
    "    # ----------------------------------------- Read Data -----------------------------------------\n",
    "    \n",
    "    def read_data(self, path):\n",
    "        df = pd.read_csv(path, usecols=['user_id', 'tweet', 'hashtags'])\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    # ----------------------------------------- Clean Data -----------------------------------------\n",
    "    \n",
    "    def clean_data(self, tweets):\n",
    "        cleaned_tweets = []\n",
    "        for text in tweets:\n",
    "            # Clean tweet\n",
    "            text = p.clean(text)\n",
    "            # Remove special characters\n",
    "            text = re.sub(r'(\\\\x(.)*)', '',text)\n",
    "            text = re.sub(r'\\\\n|\\\\t|\\\\n\\\\n', ' ', text)\n",
    "            text = re.sub(r\"b'RT|b'|b RT|b\\\"RT|b\", \"\", text)\n",
    "            text = re.sub(\"[@#$%^&*)(}{|/><=+=_:\\\"\\\\\\\\]+\",\" \",text).strip() \n",
    "            #Remove punctuation marks\n",
    "            text = \"\".join(x for x in text if x not in self.punctuation)\n",
    "            # Remove accented words\n",
    "            text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Splitting Hashtag words\n",
    "            text = \" \".join([x for x in re.split('([A-Z][a-z]+)', text) if x])\n",
    "            # Remove long spaces\n",
    "            pattern = r'^\\s*|\\s\\s*'\n",
    "            text = re.sub(pattern, ' ', text).strip()\n",
    "            # Remove numbers\n",
    "            text = re.sub('[0-9]+', '', text)\n",
    "            \n",
    "            cleaned_tweets.append(text)\n",
    "        \n",
    "        return cleaned_tweets\n",
    "    \n",
    "    \n",
    "    # ----------------------------------------- Preprocess Data -----------------------------------------\n",
    "    \n",
    "    def preprocess_data(self, tweets):\n",
    "        preprocessed_tweets = []\n",
    "        for text in tweets:\n",
    "            \n",
    "            # Remove stopwords\n",
    "            text = \" \".join(x for x in text.lower().split() if x not in self.stopword_list)\n",
    "            \n",
    "            # Text Lemmatization\n",
    "            lemmatized_words = []\n",
    "            for word in text.split():\n",
    "                word1 = self.lemmatizer.lemmatize(word, pos=\"n\")\n",
    "                word2 = self.lemmatizer.lemmatize(word1, pos=\"v\")\n",
    "                word3 = self.lemmatizer.lemmatize(word2, pos=(\"a\"))\n",
    "                lemmatized_words.append(word3)\n",
    "            text = \" \".join(x for x in lemmatized_words)\n",
    "            \n",
    "            preprocessed_tweets.append(text)\n",
    "            \n",
    "        return preprocessed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Preprocess_Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'ChopraDilpreet'</td>\n",
       "      <td>b'This #PIL filed against #Twitter for promoti...</td>\n",
       "      <td>[{'text': 'PIL', 'indices': [5, 9]}, {'text': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'jmez1010'</td>\n",
       "      <td>b\"RT @Jasleen_Kaur11: \\xf0\\x9f\\xa4\\xa3\\xf0\\x9f...</td>\n",
       "      <td>[{'text': 'Kashmir_With_India', 'indices': [85...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'jmez1010'</td>\n",
       "      <td>b\"RT @DarrenVirk: #Pakistan around partition a...</td>\n",
       "      <td>[{'text': 'Pakistan', 'indices': [16, 25]}, {'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'jmez1010'</td>\n",
       "      <td>b'RT @Harbaks21769227: #Pannun has no shame le...</td>\n",
       "      <td>[{'text': 'Pannun', 'indices': [21, 28]}, {'te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'jmez1010'</td>\n",
       "      <td>b\"RT @Jasleen_Kaur11: Pannun, a Pakistani pupp...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id                                              tweet  \\\n",
       "0  b'ChopraDilpreet'  b'This #PIL filed against #Twitter for promoti...   \n",
       "1        b'jmez1010'  b\"RT @Jasleen_Kaur11: \\xf0\\x9f\\xa4\\xa3\\xf0\\x9f...   \n",
       "2        b'jmez1010'  b\"RT @DarrenVirk: #Pakistan around partition a...   \n",
       "3        b'jmez1010'  b'RT @Harbaks21769227: #Pannun has no shame le...   \n",
       "4        b'jmez1010'  b\"RT @Jasleen_Kaur11: Pannun, a Pakistani pupp...   \n",
       "\n",
       "                                            hashtags  \n",
       "0  [{'text': 'PIL', 'indices': [5, 9]}, {'text': ...  \n",
       "1  [{'text': 'Kashmir_With_India', 'indices': [85...  \n",
       "2  [{'text': 'Pakistan', 'indices': [16, 25]}, {'...  \n",
       "3  [{'text': 'Pannun', 'indices': [21, 28]}, {'te...  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = './dataset/raw_dataset/khalistan_main.csv'\n",
    "output_path = './processed_tweets.csv'\n",
    "\n",
    "data = pre.read_data(input_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"b'This #PIL filed against #Twitter for promoting #Khalistan is a good initiative. #SikhCommunity is just trying to sa\\\\xe2\\\\x80\\\\xa6 https://t.co/w8OrEmslpu'\",\n",
       " 'b\"RT @Jasleen_Kaur11: \\\\xf0\\\\x9f\\\\xa4\\\\xa3\\\\xf0\\\\x9f\\\\x98\\\\x82Now Coming this is a next plan Pakistan  puppet pannun team\\'s. #Kashmir_With_India #ShameOnSFJ #ShameOnPannun #Khalis\\\\xe2\\\\x80\\\\xa6\"']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tweets = data.tweet.values.tolist()\n",
    "raw_tweets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This PIL filed against Twitter for promoting Khalistan is a good initiative Sikh Community is just trying to sa',\n",
       " '']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tweets = pre.clean_data(raw_tweets)\n",
    "cleaned_tweets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pil file twitter promote khalistan good initiative sikh community try sa',\n",
       " '']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_tweets = pre.preprocess_data(cleaned_tweets)\n",
    "preprocess_tweets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pil file twitter promote khalistan good initia...</td>\n",
       "      <td>[PIL, Twitter, Khalistan, SikhCommunity]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>[Kashmir_With_India, ShameOnSFJ, ShameOnPannun...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pakistan around partition weve never see efore...</td>\n",
       "      <td>[Pakistan, Gurdwara]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pannun no shame leave</td>\n",
       "      <td>[Pannun, Khalistan]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pannun pakistani puppet who work pakistan see ...</td>\n",
       "      <td>[Shame]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  pil file twitter promote khalistan good initia...   \n",
       "1                                                      \n",
       "2  pakistan around partition weve never see efore...   \n",
       "3                              pannun no shame leave   \n",
       "4  pannun pakistani puppet who work pakistan see ...   \n",
       "\n",
       "                                            hashtags  label  \n",
       "0           [PIL, Twitter, Khalistan, SikhCommunity]      0  \n",
       "1  [Kashmir_With_India, ShameOnSFJ, ShameOnPannun...      0  \n",
       "2                               [Pakistan, Gurdwara]      0  \n",
       "3                                [Pannun, Khalistan]      0  \n",
       "4                                            [Shame]      0  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2 = pd.DataFrame(columns = ['tweet','hashtags', 'label'])\n",
    "\n",
    "f2['tweet'] = pd.Series(preprocess_tweets)\n",
    "f2['hashtags'] = data.tweet.apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "f2['label'] = 0;\n",
    "\n",
    "f2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in f2.iterrows():\n",
    "    # Check if the 'B' column is empty\n",
    "    if (row['tweet'] == ''):\n",
    "        # Drop the row\n",
    "        f2 = f2.drop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pil file twitter promote khalistan good initia...</td>\n",
       "      <td>[PIL, Twitter, Khalistan, SikhCommunity]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pakistan around partition weve never see efore...</td>\n",
       "      <td>[Pakistan, Gurdwara]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pannun no shame leave</td>\n",
       "      <td>[Pannun, Khalistan]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pannun pakistani puppet who work pakistan see ...</td>\n",
       "      <td>[Shame]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pil file twitter promote khalistan good initia...</td>\n",
       "      <td>[PIL, Twitter, Khalistan, SikhCommunity]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pakistan around partition weve never see efore...</td>\n",
       "      <td>[Pakistan, Gurdwara]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pannun no shame leave</td>\n",
       "      <td>[Pannun, Khalistan]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pannun pakistani puppet who work pakistan see ...</td>\n",
       "      <td>[Shame]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>no khalistan khai garage dear sikh quick reali...</td>\n",
       "      <td>[Khalistan, Khai]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pannun not listen real sikh imaginary world do...</td>\n",
       "      <td>[Pannun, RealSikhs, SikhCommunity]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  \\\n",
       "0   pil file twitter promote khalistan good initia...   \n",
       "2   pakistan around partition weve never see efore...   \n",
       "3                               pannun no shame leave   \n",
       "4   pannun pakistani puppet who work pakistan see ...   \n",
       "5   pil file twitter promote khalistan good initia...   \n",
       "7   pakistan around partition weve never see efore...   \n",
       "8                               pannun no shame leave   \n",
       "9   pannun pakistani puppet who work pakistan see ...   \n",
       "10  no khalistan khai garage dear sikh quick reali...   \n",
       "11  pannun not listen real sikh imaginary world do...   \n",
       "\n",
       "                                    hashtags  label  \n",
       "0   [PIL, Twitter, Khalistan, SikhCommunity]      0  \n",
       "2                       [Pakistan, Gurdwara]      0  \n",
       "3                        [Pannun, Khalistan]      0  \n",
       "4                                    [Shame]      0  \n",
       "5   [PIL, Twitter, Khalistan, SikhCommunity]      0  \n",
       "7                       [Pakistan, Gurdwara]      0  \n",
       "8                        [Pannun, Khalistan]      0  \n",
       "9                                    [Shame]      0  \n",
       "10                         [Khalistan, Khai]      0  \n",
       "11        [Pannun, RealSikhs, SikhCommunity]      0  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f2.to_csv('./original/train.csv')\n",
    "f2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1172, 3)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = f2.drop_duplicates(subset=['tweet'])\n",
    "x.reset_index(inplace=True)\n",
    "x = x.drop('index', axis=1)\n",
    "x.to_csv('./original/train.csv')\n",
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
